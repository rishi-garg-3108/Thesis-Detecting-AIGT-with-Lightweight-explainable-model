

# ğŸ§  Detecting AI-Generated Text with Lightweight Explainable Models

ğŸ“ *Masterâ€™s Thesis Repository*

This repository contains the research, code, and documentation for a Master's thesis focused on **detecting AI-generated text** using **lightweight, explainable models**.
The goal is to develop methodologies that are both **efficient** and **interpretable**, distinguishing **human-written** from **AI-generated** text.

---

## ğŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ thesis/                     # Thesis-related documents
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ BERT_model/             # Fine-tuning & optimization scripts
â”‚   â”‚   â”œâ”€â”€ bert_ablation.py
â”‚   â”‚   â”œâ”€â”€ bert_optimisation.py
â”‚   â”‚   â”œâ”€â”€ finetuning_bert.py
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â””â”€â”€ multi_hyperparams_finetuning.py
â”‚   â”œâ”€â”€ run_perplexity.ipynb    # Perplexity analysis (Jupyter)
â”‚   â””â”€â”€ run_perplexity.py       # Perplexity analysis (CLI)
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### ğŸ“ Prerequisites

* **Python 3.8+**
* Recommended: Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

> ğŸ“¦ Required packages include: `torch`, `transformers`, `scikit-learn`, `pandas`, `numpy`, `tqdm`, `tabulate`, `smac`, `evaluate`, `matplotlib`.

---

## ğŸ” 1. Run Perplexity Test

Calculate perplexity of texts (human vs. AI-generated) using pre-trained models (default: `gpt2`).

```bash
python src/run_perplexity.py \
  --input path/to/dataset.csv \
  --limit 100 \
  --model_id gpt2
```

* `--input` : Path to dataset (`.csv` or `.json`)
* `--limit` : (Optional) Max samples per class
* `--model_id` : Hugging Face model (default: `gpt2`)

ğŸ““ Or use Jupyter: open `src/run_perplexity.ipynb` in Colab or Jupyter Lab.

---

## ğŸ§ª 2. Fine-tune BERT Models

### Single-run Fine-tuning

```bash
python src/BERT_model/finetuning_bert.py \
  --run_index 0 \
  --seed 42 \
  --data_path /path/to/dataset.jsonl
```

### Multi-hyperparameter Fine-tuning

```bash
python src/BERT_model/multi_hyperparams_finetuning.py \
  --seed 42 \
  --train_file /path/to/train.json \
  --val_file /path/to/val.json
```

---

## âš™ï¸ 3. Hyperparameter Optimization (SMAC)

```bash
python src/BERT_model/bert_optimisation.py
```

> ğŸ’¡ Configure `dataset_path` and `params_output_dir` inside the script before running.

---

## ğŸ“Š 4. Model Inference & Evaluation

```bash
python src/BERT_model/inference.py \
  --model_dir /path/to/saved_models \
  --test_dir /path/to/test_data
```

---

## ğŸ¤ Contribution

This repository documents the **Master's thesis work**.
For academic inquiries or collaborations, please refer to the author.

---

## ğŸ“„ License

Licensed under the **Apache License, Version 2.0**.
See the [LICENSE](LICENSE) file for details.
