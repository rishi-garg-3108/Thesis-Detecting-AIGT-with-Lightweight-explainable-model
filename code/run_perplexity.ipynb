{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Test\n",
    "\n",
    "#### Caution: The code may not work on the local system. Try google colab instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. **Change the pathname of your dataset**  \n",
    "   Update the variable `input_file` in the code cell to point to the location of your CSV/JSON file.\n",
    "\n",
    "2. **Headers of the file**  \n",
    "   The headers for the tweets can be `tweet` or `tweets`, and for the label can be `label` or `artificial`.\n",
    "\n",
    "3. **Set the limit**  \n",
    "   Adjust the variable `limit` to define how many tweets you want to process for perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_split_tweets(input_path, limit=None):\n",
    "    \"\"\"\n",
    "    Loads tweets from a CSV or JSON file, splits them into original and fake tweets\n",
    "    based on either the 'label' or 'artificial' column/key.\n",
    "    The text column can be either 'tweet' or 'tweets'.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to input file (.csv or .json)\n",
    "        limit (int): Max number of tweets to process from each class\n",
    "\n",
    "    Returns:\n",
    "        original_tweets (list of str)\n",
    "        fake_tweets (list of str)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Determine file extension and read accordingly\n",
    "    ext = os.path.splitext(input_path)[-1].lower()\n",
    "\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(input_path)\n",
    "    elif ext == \".json\":\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"Only .csv and .json files are supported.\")\n",
    "\n",
    "    # 2. Detect possible text column: either 'tweet' or 'tweets'\n",
    "    if \"tweet\" in df.columns:\n",
    "        text_col = \"tweet\"\n",
    "    elif \"tweets\" in df.columns:\n",
    "        text_col = \"tweets\"\n",
    "    else:\n",
    "        raise ValueError(\"No text column found. Must be 'tweet' or 'tweets'.\")\n",
    "\n",
    "    # 3. Detect possible label column: either 'label' or 'artificial'\n",
    "    if \"label\" in df.columns:\n",
    "        label_col = \"label\"\n",
    "    elif \"artificial\" in df.columns:\n",
    "        label_col = \"artificial\"\n",
    "    else:\n",
    "        raise ValueError(\"No label column found. Must be 'label' or 'artificial'.\")\n",
    "\n",
    "    # 4. Drop rows with missing text or label\n",
    "    df = df.dropna(subset=[text_col, label_col])\n",
    "\n",
    "    # 5. Filter tweets based on label (0 or 1)\n",
    "    original = df[df[label_col] == 0][text_col].astype(str).tolist()\n",
    "    fake = df[df[label_col] == 1][text_col].astype(str).tolist()\n",
    "\n",
    "    # 6. Apply limit if specified\n",
    "    if limit:\n",
    "        original = original[:limit]\n",
    "        fake = fake[:limit]\n",
    "\n",
    "    return original, fake\n",
    "\n",
    "\n",
    "def compute_perplexity(texts, model_id=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Computes perplexity scores using HuggingFace evaluate module.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of input texts\n",
    "        model_id (str): Pretrained model to use (default: gpt2)\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'perplexities': list of perplexity scores,\n",
    "            'mean': average perplexity\n",
    "        }\n",
    "    \"\"\"\n",
    "    metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    results = metric.compute(model_id=model_id, predictions=texts)\n",
    "\n",
    "    return {\n",
    "        \"perplexities\": results[\"perplexities\"],\n",
    "        \"mean\": np.mean(results[\"perplexities\"])\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "#   Using the functions\n",
    "# -----------------------\n",
    "\n",
    "# 1. Set your own input file path here (CSV or JSON)\n",
    "input_file = \"/content/sampled_interpretability_dataset.csv\"  # Example path\n",
    "\n",
    "# 2. Optionally limit the number of tweets per class\n",
    "limit = 100\n",
    "\n",
    "# 3. Choose a model ID (huggingface hub)\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "try:\n",
    "    # Load and split tweets\n",
    "    original_tweets, fake_tweets = load_and_split_tweets(input_file, limit=limit)\n",
    "\n",
    "    print(f\"Original tweets loaded: {len(original_tweets)}\")\n",
    "    print(f\"Fake tweets loaded: {len(fake_tweets)}\")\n",
    "\n",
    "    # Compute perplexities\n",
    "    print(\"\\nComputing perplexity for original (label=0) tweets...\")\n",
    "    orig_result = compute_perplexity(original_tweets, model_id=model_id)\n",
    "\n",
    "    print(\"Computing perplexity for fake (label=1) tweets...\")\n",
    "    fake_result = compute_perplexity(fake_tweets, model_id=model_id)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nðŸ“Š Results:\")\n",
    "    print(f\"Original Tweets (label=0): Mean Perplexity = {orig_result['mean']:.2f}\")\n",
    "    print(f\"Fake Tweets   (label=1): Mean Perplexity = {fake_result['mean']:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
